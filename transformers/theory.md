# Theory on Transformers
*Here's some resources about Theory on Transformers*


#### Attention is not all you need: Pure attention loses rank doubly exponentially with depth [`UNREAD`]

paper link: [here](http://proceedings.mlr.press/v139/dong21a/dong21a.pdf)

citation: 
```bibtex
@inproceedings{dong2021attention,
  title={Attention is not all you need: Pure attention loses rank doubly exponentially with depth},
  author={Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
  booktitle={International Conference on Machine Learning},
  pages={2793--2803},
  year={2021},
  organization={PMLR}
}
```

#### Data movement is all you need: A case study on optimizing transformers [`UNREAD`]

paper link: [here](https://proceedings.mlsys.org/paper_files/paper/2021/file/bc86e95606a6392f51f95a8de106728d-Paper.pdf)

citation: 
```bibtex
@article{ivanov2021data,
  title={Data movement is all you need: A case study on optimizing transformers},
  author={Ivanov, Andrei and Dryden, Nikoli and Ben-Nun, Tal and Li, Shigang and Hoefler, Torsten},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  pages={711--732},
  year={2021}
}
```



#### Understanding the difficulty of training transformers [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2004.08249)

citation: 
```bibtex
@article{liu2020understanding,
  title={Understanding the difficulty of training transformers},
  author={Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
  journal={arXiv preprint arXiv:2004.08249},
  year={2020}
}
```
    
   

#### Low-rank bottleneck in multi-head attention models [`UNREAD`]

paper link: [here](http://proceedings.mlr.press/v119/bhojanapalli20a/bhojanapalli20a.pdf)

citation: 
```bibtex
@inproceedings{bhojanapalli2020low,
  title={Low-rank bottleneck in multi-head attention models},
  author={Bhojanapalli, Srinadh and Yun, Chulhee and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv},
  booktitle={International conference on machine learning},
  pages={864--873},
  year={2020},
  organization={PMLR}
}
```
    
    

#### A generalization of transformer networks to graphs [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2012.09699)

citation: 
```bibtex
@article{dwivedi2020generalization,
  title={A generalization of transformer networks to graphs},
  author={Dwivedi, Vijay Prakash and Bresson, Xavier},
  journal={arXiv preprint arXiv:2012.09699},
  year={2020}
}
```
    


#### Transformer dissection: a unified understanding of transformer's attention via the lens of kernel [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/1908.11775)

citation: 
```bibtex
@article{tsai2019transformer,
  title={Transformer dissection: a unified understanding of transformer's attention via the lens of kernel},
  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Yamada, Makoto and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1908.11775},
  year={2019}
}
```
    